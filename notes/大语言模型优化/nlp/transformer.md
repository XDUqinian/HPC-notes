# Attention is all you need

这篇文章针对序列转录问题提出了一个新的完全基于注意力机制的简单网络架构 transformer，用多头自注意力取代编解码器结构中最常用的递归层，且该结构不仅适用于文本问题，还可以扩展到图像、音频、视频等领域，处理大量的输入和输出。

## 模型结构

transformer 使用普遍的序列转录架构 encoder-deconder。编码器将符号表示的输入序列 (x1,x2,…,xn) 映射到连续表示的序列 (z1,z2,…,zn)，在给定 z 的情况下解码器将逐个生成符号表示的输出序列 (y1,y2,…yn)。

------

**q1:什么是连续表示？**

传统的 one-hot 表示中，每个单词被表示为一个大型稀疏向量，该向量中只有一个元素为1，其余元素为0。

连续表示中，单词被映射为连续的向量，更加紧凑，更易于处理，通常在机器翻译中提供更好的性能。

------

在 transformer 中每一步的符号生成都是自回归的，自回归就是说在生成下一步时使用先前生成的符号作为附加输入。

但与普遍的序列转录架构不同在于，transformer的编码器解码器使用多头自注意力，独立处理，全连接层。

------

**q2:什么是point-wise？**

point-wise 是一个数学术语，表示对两个函数分别在相同的自变量取值处对函数值进行比较的过程。

此外，point-wise 也可以用来描述某种性质或行为在各个点上分别成立的情况。

transformer 中的 ponit-wise 指一个线性变换，其中每个输入元素都会被独立处理。

------

**编码器**

编码器由六个相同的层堆叠而成，每一层包括两个部分。第一部分是多头自注意力机制，第二部分是全连接前馈神经网络，子层之间使用残差连接并施加层归一化，即每层输出为 $LayerNorm(x+subayer(X))$ 。

**解码器**

解码器同样由六个相同的层堆叠组成，除了编码器的两个部分外，解码器还有第三个部分。该部分对编码器的输出执行多头自注意力。同样，子层之间采用残差连接与层归一化。还修改了解码器层次中的自注意力，通过掩码防止当前位置关注后续位置，与序列的位置嵌入相结合，确保对位置的预测只能依赖于之前的已知输出。

**注意力**

注意力可以被描述为 将一组查询 q 和一组键值对 k-v 映射到输出。其中 q、k、v 都是向量，输出是值v的加权和，权重由查询查询q与关键字k的相关性决定。

在transformer中提出 Scaled Dot-Product Attention，通过q与k的点积来描述相关性，并通过softmax函数得到权重。为了防止较大的向量维数dk将softmax推入梯度极小的区域（大概率接近于1，其余的都接近于0），点积要进行一个$\sqrt d_k$的缩放。

即  $Attention(Q,K, V) = softmax(\frac{QK^T }{\sqrt d_k})V$

**多头自注意力**

就是对同一组 QKV使用各自的权重矩阵W映射到不同的head，使用多个head进行多次自注意力，并将结果拼接起来，然后再用权重矩阵变换回一个结果的形式。

$MultiHead(Q,K, V) = Concat(head_1, ..., head_h)W^O$

$head_i = Attention(QW^Q_i,KW^K_i,VW^V_i)$

------

由于transformer没有递归和卷积，为了使模型能够利用序列的顺序，我们必须注入一些关于序列位置的信息。

论文中使用sin和cos函数解决了这个问题。

## 思考

transformer是一种全新的架构，能够处理大量的不同的输入输出，使得通用人工智能成为可能。
