# SparseGPT

Massive Language Models Can be Accurately Pruned in One-Shot

## 简介

本文首次提出了大型 ==生成式预训练 transformer(GPT)== 模型可以在只进行一次剪枝的情况下达到至少50%的稀疏性，同时模型的准确性损失最小。这种剪枝方法称为 SparseGPT，是一种非结构化剪枝。

## 介绍

现有效果最好的剪枝方法需要大量的重复训练来恢复模型精度，在大模型中这种重复训练需要的成本是昂贵的。同时，现有的一次性剪枝方法在用于百亿参数的大模型时，成本也变得很高。

概括来说，SparseGPT 将剪枝问题简化为一组极大规模的稀疏回归问题，通过一个新的近似稀疏回归求解器解决这些问题，而这个求解器足够高效。

## 原理

一次性的剪枝通常是训练后剪枝，而训练后的剪枝通常是将整个剪枝过程分层来解决的，通过每层的剪枝补偿最优化来得到整个模型剪枝的最优化。

------

**ps:为什么要分层解决？**

------

**快速权重重构**

一次性剪枝的原理在于，当我们剪掉模型的一部分时，要补偿模型的剩余部分，来弥补剪枝带来的损失。而我们当然希望通过我们的补偿，能使得剪枝带来的损失最小，这种损失可以用==交叉熵==来衡量。这个补偿的过程被称为==重构==。

当剪枝的掩码mask确定时。

根据损失最小的原则，我们其实是可以得到每行权重修剪时，该行剩余权重的最优值：

==$w^iM_i=(X_{M_i}X^T_{M_i})^{-1}X_{M_i}(w_{M_i}X_{M_i})^T$==

海森矩阵 $H=XX^T$

由于每层剪枝后的海森矩阵都不一样（我的理解是剪枝后对应位置权重置零，相当于在修改输入向量），每次为了最优化都去重新算海森矩阵花费的时间太多了。所以，就希望能实现==海森矩阵的重用==。

根据 ==OBS update== 中的推理，当m处的权重被剪枝时，剩余权重的最优调整 ==$δ_m$==，以及剪枝产生的误差 ==$ε_m$==如下：

==$δ_m=-\frac{w_m}{[H^-1]_{mm}}·H^{-1}_{:,m}$==

==$ε_m=\frac{w_m^2}{[H^{-1}]_{mm}}$==

另外，当我们通过最优调整，得到在部分mask的作用下（某个/某些权重被剪枝）的最优重构后，我们可以继续迭代地使用OBS补偿修剪剩余的权重，直到将原来的mask都用上，就得到了最优解。

但是，当我们做剪枝时，是不是可以只对剩余权重的一部分做补偿呢？当我们这么做时，我们仍然可以通过补偿来弥补剪枝带来的损失。SparseGPT只对当前剪枝列被剪枝==权重的右侧补偿==，这样就可以使用==更小的海森矩阵==，计算海森矩阵花费的时间就降低了。

![](../../../imgs/大语言模型压缩/sparseGPT.png)

**海森同步**

应该是这种方法下，海森矩阵可以并行计算？

具体原理暂时没看懂。

通过删去原海森矩阵的第一行和第一列，可以算出后续的海森矩阵。

**自适应掩码选择**

选掩码就是选我要剪哪些东西。

因为涉及到权重的重构，现有研究表明==选掩码的同时考虑重构==会有更好的结果，因此本文就提出了一种自适应掩码选择方法。

使用==迭代块==的方式，每次随机选择128列（根据海森矩阵的对角线值和剪枝产生的误差$ε_m$）生成掩码，然后做重构，再随机选择、再重构……以此类推。

## 思考

这篇论文的思路就是，通过只补偿部分剩余权重来减小海森矩阵的大小，以此加速海森矩阵的计算时间，加速one-shot剪枝。

但是只补偿部分权重可能会使得补偿的效果没原来那么好。

且该方法基于梯度来剪枝，存海森矩阵需要较大的存储空间。

能否通过别的数学模型做别的加速？

能否通过找到参数之间的关系，进行有针对性的补偿，而不是泛泛地去补偿？
