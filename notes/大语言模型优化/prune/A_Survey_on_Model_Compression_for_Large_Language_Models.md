# 大语言模型压缩技术综述

本文介绍了大语言模型压缩的方法，探讨了评估大语言模型压缩的策略和指标，以及大语言模型压缩领域目前存在的问题。可以根据这篇综述中提到的论文进行进一步实验和研究。

## 内容概括

### 模型压缩方法

#### 剪枝

剪枝就是通过裁剪模型中不重要或冗余的部分来进行模型压缩。

**非结构化剪枝**

忽视大预言模型整体结构，修剪单个参数得到不规则的稀疏结构。这种不规则的稀疏性使得我们需要对剪枝后的模型采取特殊的压缩技术以及高效的存储与计算技术。非结构化剪枝通常需要对模型进行多次再训练来恢复模型精度。

**结构化剪枝**

通过移除整个结构组件（如神经元、通道或层）来简化模型。一次以整组权重为目标，在保持整体模型结构的前提下降低模型复杂性与内存使用。

#### 知识蒸馏 KD

通过将知识从复杂的教师模型转移到简单的学生模型实现模型压缩。

**Standard KD**

使学生模型能学习多个大语言模型的共同知识，如输出分布和特征信息。

**EA KD**

超越了标准KD传递普通知识的范畴，还能提炼出大语言模型独特的应急能力。

上下文学习 使用包含任务描述和几个任务示例的结构化自然语言进行提示，使得大语言模型可以在不需要显式梯度更新的情况下掌握和执行新任务。

链式思考 将中间推理步骤整合到提示中。

指令遵循 完全基于阅读任务描述能力，提高语言模型执行新任务的能力。

#### 量化

量化将传统的浮点数表示转换为整数或其他离散形式，显著地降低存储需求和计算复杂性。

虽然存在精度损失，但巧妙设计的量化方法可以在精度损失最小的情况下实现大语言模型的压缩。

**训练前量化**

在训练过程中集成量化，使得大语言模型能在训练时就适应低精度表示，增强其处理量化造成的精度损失的能力。

**微调量化**

在微调恢复精度的过程中进行量化，确保量化后的大语言模型仍然能够保持性能，取得模型压缩与性能保持之间的平衡。

**训练后量化**

训练后量化主要是为了降低大语言模型的存储和计算复杂性，而这些需求不必对大语言模型架构进行修改或对大语言模型进行微调再训练。这种量化方法很直接，但却会引入一定程度的精度损失。PTQ中，某些方法只量化大语言模型的权重，以提高效率并减少计算需求，但大多数试图同时量化权重和激活函数。

#### 低秩分解

通过两个或更多更小的矩阵来逼近原权重矩阵。

核心思想涉及找到将大权重矩阵W分解成两个矩阵U和V的因式分解，使得W≈UV，其中U是m×k矩阵，V是k×n矩阵，其中k远远小于m和n。

### 指标和基准

#### 指标

- 参数数量
- 模型尺寸
- 压缩比
- 推理时间
- 浮点计算数

#### 基准

- Common benchmarks
- HULK
- ELUE

### 挑战与未来方向

- 专业的评价基准
- 性能与尺寸的权衡
- 大语言模型动态压缩
- 模型压缩的可解释性

## 思考

本文介绍的这些方法准备依次做复现，为后续横向课题的开展打基础。